Chapter 6: Core Concepts of Imaging Informatics
6.1 Standards
DICOM
The Digital Imaging and Communications in
Medicine (DICOM) standard
(http://dicom.nema.org) is the international
standard that specifies protocols for display,
transfer, storage, and processing of medical
images. The DICOM standard applies to
storage of both pixel-based image data and
metadata. The metadata, located in the
“DICOM header” of the image, contains
information about the image, series, exam,
patient, imaging facility, and scanner. The data
are organized into separate fields, each of
which has a unique identifier so that it can be
queried directly. DICOM transactions enable
data to be queried, retrieved, and transmitted
between systems in an organized fashion. They
also allow for information about an order to be
transmitted between the radiology information
system (RIS) and the modality (e.g., the CT,
MR, or ultrasound machine) rather than having
to be manually entered by the technologist and
risking incorrect data entry.
Standard DICOM data elements are required to
contain specific information while private data
elements can be defined by the vendor. To
enable interoperability between systems,
vendors who implement products that use
DICOM are expected to provide customers
with conformance statements that detail their
use of the DICOM standard.
HL7
HL7 (http://www.hl7.org) is the international
standards organization responsible for
developing and maintaining standards for the
exchange, integration, sharing, and retrieval of
medical information (i.e., nonimage data). The
primary HL7 standards are the ones most
frequently used to achieve systems
interoperability.
The HL7 V2 messaging standard is generally
considered to be the most widely implemented
healthcare-related standard in the world. This
text- based standard facilitates the exchange of
medical data by enabling interoperability
between many types of electronic medical
systems that need to communicate. HL7 V3,
while more human-readable, has been less
widely adopted in the industry because of its
increased complexity. The newer HL7 Fast
Healthcare Interoperability Resources (FHIR®)
standard allows software developers to use
internet transactions to exchange medical data
between systems, increasing the potential for
data exchange between systems.
Ontologies
Ontologies are formal collections of terms and
their inherited or causal relationships. RadLex
(http://www. radlex.org) is the largest
radiology-specific lexicon. It contains more than
68,000 terms that describe imaging anatomy,
procedures, and pathology. A special portion of
the RadLex ontology, the RadLex Playbook,
defines standard imaging exam names,
descriptions, and codes. The RadLex Playbook
has been merged with LOINC (Logical
Observation Identifiers Names and Codes), the
international standard nomenclature for health
measurements, observations, and documents.
6.2 The Reading Room Environment
PACS
The PACS (picture archiving and
communications system) is the radiologist’s
primary tool for imaging viewing and
interpretation. Basic components of PACS
include a workstation, display, short-term
storage, and long-term archive. PACS
communicates with imaging modalities using
DICOM transactions, and with the RIS and/or
EMR using HL7 transactions that are translated
to and from DICOM. Unlike original PACS
implementations that required a physical
workstation to run, the modern PACS can be
entirely web-based and accessible on mobile
devices as well as on desktop thin clients.
VNA
The development of the vendor-neutral archive
(VNA) allows data to be stored in a central
archive that may support viewers for multiple
types of DICOM images (e.g., radiology,
cardiology, operating room, etc.), as well as for
non-DICOM data, including photographs and
pathology slides. Enterprise imaging relies
heavily on VNA technology to facilitate
dissemination, viewing, and storage of medical
imaging data beyond radiology. Determining
how best to format and exchange the metadata
(e.g., patient information, body part, date of
acquisition, etc.) accompanying a non-DICOM
image is a major challenge in enterprise
imaging.
RIS
The radiology information system (RIS) is a
software application that manages all aspects of
an imaging exam, including order
reconciliation, patient scheduling and tracking,
communication with modalities and PACS,
reporting, results notification, and billing. The
RIS may be a standalone application or a
component of the electronic medical record
(EMR) application. Both PACS and RIS can be
used to drive clinical workflow.
Image Displays
The ACR-AAPM-SIIM technical standard
recommends that ideal reading room ambient
lighting fall in the range of 25 to 50 lux. This
level of lighting is similar to standing under a
street light at night in dark surroundings. The
maximum gray value luminance for diagnostic
monitors is recommended to be at least 350
cd/m2 for nonmammographic interpretation
and 420 cd/m2 for mammographic
interpretation. By way of reference, topperforming flat screen televisions on the market
in 2017 have a peak luminance upwards of 400
cd/m2.
Compression
Compression is used to decrease image file size
to speed up transfer and decrease storage
requirements. Lossless compression is achieved
by decreasing redundant image information
(e.g., the black background of a CT image).
Because image content is preserved, lossless
compression can only reduce image file size by
approximately 3:1. Lossy compression allows
for more substantial image size compression
(on the order of 10:1) by irreversibly discarding
unnecessary or minimally important image
information without significantly
compromising diagnostic quality.
Ergonomics
Like all individuals who spend many hours
working on a computer, radiologists are
susceptible to repetitive strain injuries (RSI).
For example, carpal tunnel syndrome
(involving the median nerve) often occurs due
to dorsiflexion of the wrist from upward
angulation of the wrist while typing. Cubital
tunnel syndrome (involving the ulnar nerve)
can occur due to RSI at either the wrist or the
elbow. DeQuervain tenosynovitis occurs
secondary to RSI of the thumb.
Workstation configurations that promote a
neutral body position with the forearm, wrist,
and hand parallel to the floor, lumbar support,
and appropriate distance between the user and
the display can help to decrease the incidence
of RSI among radiologists.
6.3 From Orderto Report:Workflow
Considerations
Workflow Steps
Medical imaging depends on interoperability
between many systems, including PACS, RIS,
EMR and imaging modalities, as data are
transferred via DICOM and HL7 transactions.
The process begins with an order placed in the
EMR. HL7 transactions communicate the order
to the RIS (if it is a separate system). The RIS
communicates orderinformation to the relevant
imaging modality (meaning the machine) via
the DICOM Modality Work List, and the
modality communicates with the PACS via
DICOM transactions. The radiologist views the
images on PACS and dictates the report using
voice recognition. The reporting software then
sends the report to the RIS and EMR via HL7
transactions.
Downtime Procedures
Downtime procedures include disaster
recovery (DR) and business continuity (BC)
procedures. DR policies direct activities that
should be followed in the event of a disaster,
such as a large-scale, unexpected, highly
disruptive event, whether natural or human in
origin.
DR policies typically include a description of
off-site data backup systems, including the
frequency of backup cycles, and the steps
required to restore critical data in the event of a
disruption. BC policies refer to the necessary
systematic precautions, backups, and failover
routines required to continue to care for
patients when a system failure (such as a power
outage) occurs under otherwise routine
working conditions.
Radiology systems are considered to be highavailability (HA) systems. HA systems must have
the ability to perform automated recovery and
failover operations in the event of service
disruption. The uptime expectations of an HA
system can be expressed as a “number of nines.” For
example, PACS is generally expected to perform at
“four nines”, or 99.99% uptime, which translates to
no more than approximately 50 minutes of
downtime a year. Fault tolerance (FT) refers to the
ability of a system to continue to function if one of
its components fails. To avoid single points of
failure, redundancy is built into essential
components of a system (e.g., servers, network
connections, data archives, etc.) to achieve a high FT.
6.4 Data Privacy and Security
De-identification of Images
De-identification involves removing protected
health information (PHI), as defined by HIPAA,
from an imaging examination such that the
identity of the patient cannot be directly
determined based on information contained in
the images or the metadata. However, deidentified images may contain information that
enables an approved entity to identify the
patient using a key. In contrast, anonymization
involves removing all PHI and other
identifiable data from an imaging examination
such that the identity of the patient is not
revealed and cannot can be re-established in the
future. PHI contained in the metadata can
typically be removed via automated deidentification processes. In some cases,
“burned-in” PHI (such as in ultrasound images)
also must be removed to fully de-identify
medical images. Because the contours of a
patient’s face can be reconstructed from CT or
MRI of the head, imaging that includes the face
is also considered PHI.
De-identification of Report Text
De-identification of report data is less
straightforward than de-identification of image
data, because PHI does not occur in radiology
reports in the same form or with the same
consistency. De- identification of report data
often requires manual review or application of
specialized algorithms.
Tools for de-identification of other medical text,
such as encounter notes or progress notes,
generally do not work as well for radiology
reports, because of the less frequent appearance
of PHI in radiology reports compared to medical
text.
Cybersecurity
Ransomware attacks occur when bad actors
encrypt files and systems and demand a ransom
in exchange for the decryption key. Such attacks
can be catastrophic to a radiology practice and
its associated hospital or health system. Typical
downtime procedures may not be sufficient for
recovery. Instead, a long-term analog (e.g.,
paper-based) workflow may be necessary to
maintain business continuity while
compromised files and systems are isolated and
data recovery is attempted. Eventually, any
recovered data will have to be reconciled with
new data collected after the attack.
6.5 Image Post-Processing
Post-processing refers to image transformations
performed after acquisition. These
transformations may occur before image
display, interpretation, or quantitative analysis.
Post-processing includes techniques such as
image segmentation, registration, and threedimensional (3-D) post-processing using
maximum intensity projections (MIPs),
multiplanar reformats (MPRs), or volume
rendering.
Segmentation involves isolating or extracting a
region of interest from an image or extracting a
subset of images from an image stack for further
analysis. For example, segmentation of gray matter
and white matter from MRI of the brain may be the
first step to a more advanced analysis of atrophy in
neurodegenerative disorders.
Image registration involves aligning one image set
onto the coordinate space of another image set to
allow a more direct comparison of the two image
sets. Deformations can be rigid (e.g., rotation,
translation, reflection), affine (e.g., shearing,
scaling), or elastic. Elastic deformation involves
local warping of an image to better align the target
image with the reference image. Elastic
deformation is one type of image registration that
can accommodate changes such as patient position,
lung expansion, or soft tissue shape changes in
aligning image sets.
3-D post-processing is a required component of CT
and MR angiography and can supplement other
advanced imaging, such as cardiac MRI and brain
MRI for tumor analysis. Simple 3-D postprocessing, such as MPRs, MIPs, and volume
rendering, can be performed on most modern
versions of PACS. More elaborate analysis such as
curved planar reformats (CPRs), functional
analysis, and cinematic rendering, however, may
require additional thin-client software and may be
supported by a dedicated team of experts, such as a
3-D lab.
6.6 Artificial Intelligence
Artificial intelligence (AI) is the field of
computer science that gives computers the
ability to mimic human intelligence. Machine
learning (ML) is a subfield of AI that enables
computers to learn a task without an explicit
set of instructions. Deep learning (DL) uses multilayered neural networks with weighted connections
to analyze data, and works with
both images and text. AI serves as an adjunct to the
human radiologist; at present, few autonomous AI
applications exist for radiology.
Supervised and Unsupervised Learning
Supervised learning exposes an algorithm
to a labeled set of training data and then
evaluates how well the resulting model
predicts labels on a different set of test
data. It is important that the test set data
not overlap the training data, so that
model performance is not artificially
exaggerated.
Unsupervised learning exposes an
algorithm to a set of data without predefined labels or categories and expects
the algorithm to organize the data.
Unsupervised learning models must be
validated and may not perform as well as
supervised models without further
training.
Training AI Models
Generating training data for radiology
requires experts to label images or text,
which is time- and resource- intensive. A
common pitfall in model training is
overfitting the model to the data, such that
it performs very well on similar data (e.g.,
at the organization where it was trained),
but does not perform as well on data that
are different in some way (e.g., data from
another organization). Labeling images is
task specific and can be as simple as
assigning a label to an entire image or
study (e.g., “normal”, “abnormal”), or it
may require an expert to use segmentation
tools to identify an anatomic structure or
disease process. As with de-identification,
labeling text data or workflow data
requires different tools than labeling image
data.
Deep Learning forImages
Deep learning models use neural networks
with an input layer, multiple hidden layers,
and an output layer to perform inference or
make predictions. Convolutional neural
networks and recurrent neural networks are DL
algorithms often used in radiology applications.
Image data are pre-processed before use for training
or inference to make them suitable for input to an
AI algorithm; this may include rescaling, cropping,
denoising, histogram equalization, and often
downsampling (sometimes by as much as 10x). This
decrease in image resolution may reduce
the conspicuity of subtle findings in images
unless sufficient training examples are provided.
Natural Language Processing
Natural language processing (NLP) is the analysis
of human language data. Common uses in
radiology reports include detection of critical
findings or follow-up recommendations,
longitudinal lesion monitoring, assessment of
report compliance with practice requirements, or
radiology-pathology correlation. Text documents
are also pre-processed like images, with removal of
stop words and punctuation, conversion to lower
case, and tokenization. Text data can be tokenized
by converting words and phrases to numeric
representations for input into deep learning
models through a process known as embedding.
Deployment Challenges
Major challenges in deploying AI for radiology
include understanding how the “black box” model
produces its results, ensuring that the model
performs reliably in all potential applied settings
and conditions, and efficiently integrating the
model into the clinical workflow. Once deployed,
models should be monitored to identify data drift,
in which model performance degrades over time
due to gradual changes in the data it processes.
Additionally, the way that radiologists interact
with AI should be monitored to guard against
automation bias, in which the computer is always
assumed to be more correct than the human
practitioner.
Bias in AI
Bias in AI can occur due to the training data, the
model architecture, or the conclusions drawn by
end users based on model outputs. Statistical
bias results when a model does not represent
the true features of the population. One
example of statistical bias is sampling bias, in
which the data used to train the model do not
represent the patients to whom the model will
ultimately be applied. Social bias has the
potential to exacerbate health disparities by
adversely affecting underrepresented
populations. For example, a model may predict
better health outcomes in a patient population
that does not use healthcare resources as
frequently, but it may overlook the fact that the
use or lack thereof is related to access to care
rather than their state of health